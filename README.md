ğŸ” Task 6: K-Nearest Neighbors (KNN) Classification

This project demonstrates how to implement the K-Nearest Neighbors algorithm to classify the Iris dataset, focusing on normalization, model evaluation, and decision boundary visualization.

ğŸ¯ Objective

To understand and apply the KNN algorithm for classification problems using:

Scikit-learn

Pandas

Matplotlib

ğŸ“ Dataset Used

Iris Dataset (from sklearn.datasets.load_iris())

ğŸ› ï¸ What You'll Learn

How instance-based learning works

Importance of normalization

Role of Euclidean distance

How to choose the optimal K value

Visualization of decision boundaries

ğŸš€ How to Run This Project

Clone this repository or download the notebook.

Install the required packages:

bash

pip install pandas matplotlib seaborn scikit-learn

Run the notebook:

bash

jupyter notebook task6.ipynb

ğŸ§ª Workflow Steps

Load the Iris dataset.

Normalize features using StandardScaler.

Split the data into training and test sets.

Train the KNN Classifier using different values of K.

Evaluate the model using:

Accuracy score

Confusion matrix

Classification report

Visualize model performance and decision boundaries using matplotlib.

ğŸ“ˆ Output Highlights

Accuracy vs K value plot

Confusion matrix and metrics report

Decision boundary graph using 2D features
